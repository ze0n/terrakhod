{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/2c/4572e2e495341e667c89b490ad18ea71a5f9e9fafca06109a9c7db22848b/h5py-2.8.0-cp35-cp35m-win_amd64.whl (2.3MB)\n",
      "Requirement already satisfied: six in c:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from h5py)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from h5py)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, InputLayer, LeakyReLU, Dropout\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator # Usefull thing. Read the doc.\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (90,180)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = imread(join('../DatasetGeneration/SampleData/1', 'drum.0023.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imgs_and_labels(\n",
    "    labels_dirname='../DatasetGeneration/SampleData/1', \n",
    "    img_dirname='../DatasetGeneration/SampleData/1', \n",
    "    debug=False):\n",
    "  \"\"\"\n",
    "  Returns a numpy array of images, a numpy array of steering angles, and an array of labels\n",
    "  \"\"\"\n",
    "  # Read csv\n",
    "  csv_file = join(labels_dirname, 'data.csv')\n",
    "  df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "  # Read images\n",
    "  img_files = [f for f in listdir(img_dirname) if f.endswith(\".jpg\") and isfile(join(img_dirname, f))]\n",
    "    \n",
    "  imgs = []\n",
    "  labels = []\n",
    "  for img_file in img_files:\n",
    "    img = imread(join(img_dirname, img_file))\n",
    "    # The original images are 1024x1280, way too big for the raspberry\n",
    "    # I resize as in donkeycay: https://github.com/wroscoe/donkey/blob/dev/donkeycar/util/img.py\n",
    "    # Obtained shape is (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # Take only lower part\n",
    "    shape = img.shape\n",
    "    img = img[int(shape[0]/2):]\n",
    "    img_resized = resize(img, IMG_SIZE)\n",
    "    # Convert to gray scale; obtained shape is (IMG_SIZE, IMG_SIZE)\n",
    "    img_gray = rgb2gray(img_resized)\n",
    "    # Need to add a dimension in order to get shape (IMG_SIZE, IMG_SIZE, 1), because the CNN\n",
    "    # needs data with 3 or more dimensions\n",
    "    img_reshaped = img_gray[..., np.newaxis]\n",
    "    imgs.append(img_reshaped)\n",
    "    # Get the corresponding label\n",
    "    label = df.loc[img_file,'Label']\n",
    "    labels.append(label)\n",
    "\n",
    "    # I show the images for debugging purposes\n",
    "    if ( debug ):\n",
    "      imshow(img_gray)\n",
    "      plt.title(img_file + \": \" + label)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "  imgs_np = np.array(imgs)\n",
    "                \n",
    "  return imgs_np, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(output_size):\n",
    "  \"\"\"\n",
    "  Let's start with a simple model\n",
    "  \"\"\"\n",
    "\n",
    "  model = keras.models.Sequential()\n",
    "  # Define here your model\n",
    "\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding=\"same\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1), activation='relu'))  # first layer needs to define \"input_shape\"\n",
    "  #model.add(LeakyReLU(0.1))\n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))    \n",
    "  model.add(Conv2D(filters=64, kernel_size=2, padding=\"same\", activation='relu'))\n",
    "  #model.add(LeakyReLU(0.1))\n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))    \n",
    "  model.add(Conv2D(filters=128, kernel_size=2, padding=\"same\", activation='relu'))\n",
    "  #model.add(LeakyReLU(0.1))\n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "      \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(500, activation='relu'))\n",
    "  #model.add(LeakyReLU(0.1))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(output_size, activation='softmax'))  \n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, imgs, labels, model_name=None):\n",
    "\n",
    "  imgs_train, imgs_val, labels_train, labels_val = train_test_split(imgs, labels, test_size=0.1)\n",
    "\n",
    "  model.compile(\n",
    "      loss='categorical_crossentropy', \n",
    "      metrics=['accuracy'],\n",
    "      optimizer=Adam()\n",
    "  )\n",
    "\n",
    "  # Choose optimizer, compile model and run training\n",
    "  earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=0,\n",
    "                                patience=2,\n",
    "                                mode='auto')\n",
    "\n",
    "  datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             zoom_range=0.1\n",
    "                            )\n",
    "\n",
    "  history = model.fit_generator(\n",
    "      datagen.flow(imgs_train, labels_train, batch_size=BATCH_SIZE),\n",
    "      validation_data=(imgs_val, labels_val),\n",
    "      epochs=EPOCHS, \n",
    "      steps_per_epoch=len(imgs_train) // BATCH_SIZE,\n",
    "      callbacks=[ModelCheckpoint(model_name + \"-{epoch:02d}-{val_loss:.2f}.hdf5\", save_best_only=True),\n",
    "                ] if model_name is not None else [],\n",
    "      #callbacks=[earlyStopping],\n",
    "      shuffle=True\n",
    "      )  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "# Loads ims and labels\n",
    "imgs, labels = load_imgs_and_labels(debug=False)\n",
    "nb_different_labels = len(set(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_as_integers = label_encoder.fit_transform(labels)\n",
    "labels_one_hot_encoded = to_categorical(labels_as_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialises the model\n",
    "model = get_model( nb_different_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 4s 592ms/step - loss: 2.4629 - acc: 0.1512 - val_loss: 2.4695 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 2.3684 - acc: 0.1475 - val_loss: 2.3877 - val_acc: 0.1200\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 2.3602 - acc: 0.1317 - val_loss: 2.5396 - val_acc: 0.1200\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.2997 - acc: 0.1512 - val_loss: 2.4653 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 2.3116 - acc: 0.1458 - val_loss: 2.4733 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 2.2732 - acc: 0.1460 - val_loss: 2.4557 - val_acc: 0.2400\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.3038 - acc: 0.1414 - val_loss: 2.4086 - val_acc: 0.2000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 2.2773 - acc: 0.1475 - val_loss: 2.4238 - val_acc: 0.2400\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2.2428 - acc: 0.2223 - val_loss: 2.4030 - val_acc: 0.1600\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 2.3206 - acc: 0.1227 - val_loss: 2.3581 - val_acc: 0.1600\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 2.2675 - acc: 0.1938 - val_loss: 2.4879 - val_acc: 0.2000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 2.2606 - acc: 0.1823 - val_loss: 2.3559 - val_acc: 0.1600\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 2.2794 - acc: 0.1497 - val_loss: 2.3791 - val_acc: 0.0800\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 2.2317 - acc: 0.1984 - val_loss: 2.3805 - val_acc: 0.1200\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 2.2541 - acc: 0.1587 - val_loss: 2.4030 - val_acc: 0.1600\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 2.1937 - acc: 0.2021 - val_loss: 2.3440 - val_acc: 0.1600\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.2461 - acc: 0.1771 - val_loss: 2.3551 - val_acc: 0.2000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.1595 - acc: 0.2051 - val_loss: 2.4236 - val_acc: 0.2000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 2.2354 - acc: 0.2403 - val_loss: 2.3422 - val_acc: 0.1600\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 235ms/step - loss: 2.2004 - acc: 0.1938 - val_loss: 2.3121 - val_acc: 0.1600\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 2.2072 - acc: 0.2081 - val_loss: 2.2978 - val_acc: 0.2000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 221ms/step - loss: 2.1613 - acc: 0.2135 - val_loss: 2.2831 - val_acc: 0.1600\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 234ms/step - loss: 2.1817 - acc: 0.1935 - val_loss: 2.2568 - val_acc: 0.3200\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 245ms/step - loss: 2.1234 - acc: 0.2188 - val_loss: 2.2416 - val_acc: 0.3200\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 2.1356 - acc: 0.2018 - val_loss: 2.2971 - val_acc: 0.2800\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0831 - acc: 0.225 - 0s 47ms/step - loss: 2.0700 - acc: 0.2425 - val_loss: 2.2673 - val_acc: 0.2800\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 2.1286 - acc: 0.2240 - val_loss: 2.2909 - val_acc: 0.2000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 2.0768 - acc: 0.2545 - val_loss: 2.2129 - val_acc: 0.2400\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2.0658 - acc: 0.2365 - val_loss: 2.2634 - val_acc: 0.1200\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 2.0698 - acc: 0.1961 - val_loss: 2.2578 - val_acc: 0.1200\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9772 - acc: 0.2822 - val_loss: 2.3487 - val_acc: 0.0800\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 2.0462 - acc: 0.2656 - val_loss: 2.1332 - val_acc: 0.1600\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 2.0091 - acc: 0.2470 - val_loss: 2.2158 - val_acc: 0.0800\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.9898 - acc: 0.2708 - val_loss: 2.3713 - val_acc: 0.1200\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 2.0036 - acc: 0.2777 - val_loss: 2.0972 - val_acc: 0.2400\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.9435 - acc: 0.2664 - val_loss: 2.2885 - val_acc: 0.2000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0110 - acc: 0.2396 - val_loss: 2.1003 - val_acc: 0.2800\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.9640 - acc: 0.2747 - val_loss: 2.1526 - val_acc: 0.2800\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.9373 - acc: 0.2994 - val_loss: 2.2391 - val_acc: 0.2400\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9620 - acc: 0.2817 - val_loss: 2.1548 - val_acc: 0.2800\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 1.8147 - acc: 0.2865 - val_loss: 2.0820 - val_acc: 0.3200\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 1.9510 - acc: 0.2792 - val_loss: 2.0803 - val_acc: 0.3200\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 1.9603 - acc: 0.2665 - val_loss: 2.0218 - val_acc: 0.3200\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 1.8711 - acc: 0.3229 - val_loss: 2.0961 - val_acc: 0.3200\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9033 - acc: 0.2606 - val_loss: 2.0824 - val_acc: 0.3200\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.9386 - acc: 0.3125 - val_loss: 2.1318 - val_acc: 0.3200\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.9386 - acc: 0.2725 - val_loss: 2.1795 - val_acc: 0.2400\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.8683 - acc: 0.3203 - val_loss: 2.1019 - val_acc: 0.2000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.9358 - acc: 0.2732 - val_loss: 2.1900 - val_acc: 0.2400\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.8939 - acc: 0.2994 - val_loss: 2.1491 - val_acc: 0.2400\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.8643 - acc: 0.3073 - val_loss: 2.1174 - val_acc: 0.1200\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.7993 - acc: 0.3034 - val_loss: 2.1542 - val_acc: 0.2800\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.8422 - acc: 0.2979 - val_loss: 2.0869 - val_acc: 0.2800\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.8909 - acc: 0.2500 - val_loss: 2.1213 - val_acc: 0.2000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 1.7666 - acc: 0.3241 - val_loss: 2.2105 - val_acc: 0.2000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.9343 - acc: 0.2380 - val_loss: 2.1904 - val_acc: 0.2400\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.8737 - acc: 0.2725 - val_loss: 2.1874 - val_acc: 0.3200\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.7495 - acc: 0.3333 - val_loss: 2.1543 - val_acc: 0.2800\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.9224 - acc: 0.2869 - val_loss: 2.1362 - val_acc: 0.2000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7721 - acc: 0.3353 - val_loss: 2.0808 - val_acc: 0.2400\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.7606 - acc: 0.3490 - val_loss: 2.0682 - val_acc: 0.3200\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.8541 - acc: 0.2952 - val_loss: 2.1151 - val_acc: 0.1600\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.8304 - acc: 0.2865 - val_loss: 2.0652 - val_acc: 0.3200\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.7048 - acc: 0.3727 - val_loss: 2.0754 - val_acc: 0.2800\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 1.7735 - acc: 0.3338 - val_loss: 2.2953 - val_acc: 0.2400\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.8056 - acc: 0.3166 - val_loss: 2.1163 - val_acc: 0.2800\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 1.8465 - acc: 0.2865 - val_loss: 2.1145 - val_acc: 0.2800\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 1.7957 - acc: 0.3277 - val_loss: 2.1720 - val_acc: 0.2800\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.8862 - acc: 0.3159 - val_loss: 2.1087 - val_acc: 0.2400\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8671 - acc: 0.293 - 0s 31ms/step - loss: 1.8405 - acc: 0.3229 - val_loss: 2.1729 - val_acc: 0.2800\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 1.7660 - acc: 0.3211 - val_loss: 2.2799 - val_acc: 0.3200\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.8320 - acc: 0.2987 - val_loss: 2.1843 - val_acc: 0.2800\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.7915 - acc: 0.3385 - val_loss: 2.1672 - val_acc: 0.3200\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 1.7724 - acc: 0.3442 - val_loss: 2.1017 - val_acc: 0.2800\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.7950 - acc: 0.3177 - val_loss: 2.1335 - val_acc: 0.2400\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.8031 - acc: 0.2919 - val_loss: 2.1761 - val_acc: 0.2400\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.7814 - acc: 0.3623 - val_loss: 2.1544 - val_acc: 0.2400\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 1.7414 - acc: 0.3361 - val_loss: 2.1548 - val_acc: 0.3600\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.7737 - acc: 0.3204 - val_loss: 2.0893 - val_acc: 0.3200\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.7729 - acc: 0.3121 - val_loss: 2.0630 - val_acc: 0.3600\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.7417 - acc: 0.3593 - val_loss: 2.0806 - val_acc: 0.2400\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.8366 - acc: 0.3121 - val_loss: 2.1106 - val_acc: 0.1600\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.6965 - acc: 0.3542 - val_loss: 2.1695 - val_acc: 0.2000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.7203 - acc: 0.3218 - val_loss: 2.2813 - val_acc: 0.2000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.8340 - acc: 0.3024 - val_loss: 2.2250 - val_acc: 0.1600\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.7470 - acc: 0.322 - 0s 32ms/step - loss: 1.7468 - acc: 0.3159 - val_loss: 2.2046 - val_acc: 0.1600\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 1.6554 - acc: 0.3877 - val_loss: 2.2287 - val_acc: 0.2400\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7596 - acc: 0.3593 - val_loss: 2.1675 - val_acc: 0.1600\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.6791 - acc: 0.3542 - val_loss: 2.1840 - val_acc: 0.2400\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 1.7432 - acc: 0.3009 - val_loss: 2.1944 - val_acc: 0.2800\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.7035 - acc: 0.3899 - val_loss: 2.1466 - val_acc: 0.2000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.6733 - acc: 0.4296 - val_loss: 2.1015 - val_acc: 0.2400\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 1.8261 - acc: 0.2822 - val_loss: 2.0902 - val_acc: 0.2800\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.7216 - acc: 0.3698 - val_loss: 2.0903 - val_acc: 0.2400\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.7010 - acc: 0.3428 - val_loss: 2.0342 - val_acc: 0.2400\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 1.6563 - acc: 0.3555 - val_loss: 2.0210 - val_acc: 0.3200\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.8015 - acc: 0.3286 - val_loss: 2.1303 - val_acc: 0.2800\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.6899 - acc: 0.3749 - val_loss: 2.0310 - val_acc: 0.2800\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.7086 - acc: 0.3802 - val_loss: 2.1377 - val_acc: 0.2400\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.7063 - acc: 0.3508 - val_loss: 2.0789 - val_acc: 0.3200\n"
     ]
    }
   ],
   "source": [
    "train_model(model, imgs, labels_one_hot_encoded, model_name='Simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
